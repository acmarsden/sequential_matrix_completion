{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.5/site-packages/theano/tensor/signal/downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import scipy.sparse as sp\n",
    "from theano import sparse\n",
    "import lasagne\n",
    "import time\n",
    "import scipy.stats as stats\n",
    "from collections import OrderedDict\n",
    "import sys \n",
    "sys.setrecursionlimit(50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate a matrix from a multivariate normal distribution with low-rank covariance matrix\n",
    "KTRUE = 5\n",
    "K = 20\n",
    "N = 100\n",
    "D = 50\n",
    "maxit = KTRUE*(N+D-KTRUE)\n",
    "#maxit = N*D\n",
    "\n",
    "np.random.seed(seed=10)\n",
    "\n",
    "#c0np = np.random.gamma(1,1)\n",
    "#gamma0np = np.random.gamma(1,1)\n",
    "#gammanp = np.random.gamma(1,1)\n",
    "#rnp = np.random.gamma(gamma0np/KTRUE, c0np, size=(KTRUE))\n",
    "#w = np.zeros((D,KTRUE))\n",
    "#for k in range(KTRUE):\n",
    "#    print(gammanp*rnp[k])\n",
    "#    print(gammanp)\n",
    "#    w[:,k] = np.random.gamma(gammanp*rnp[k]+1e-20, gammanp+1e-20, size = (D))\n",
    "    \n",
    "\n",
    "w   = np.random.uniform(low=0.0, high=1.0, size=(D,KTRUE))\n",
    "#Introduce some complexity into w\n",
    "#w     = np.random.beta(a = 2, b = 5, size = (D,KTRUE))\n",
    "#maskw = stats.bernoulli.rvs(0.8, size=(D,KTRUE))\n",
    "#w     = np.multiply(w,maskw)\n",
    "var = 0.1\n",
    "covnp = w.dot(w.T)+var*np.eye(D)\n",
    "\n",
    "Mnp = np.random.multivariate_normal(np.zeros(D), covnp, N).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "725"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_w_init = np.random.uniform(low=0.01, high=2.0, size=(D,K))\n",
    "random_r_init = np.random.uniform(low = 0.01, high = 2.0, size = K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use Theano for our model\n",
    "srng = T.shared_randomstreams.RandomStreams(seed=120)\n",
    "\n",
    "#Define Theano Variables\n",
    "Shared = lambda shape,name: theano.shared(value = np.ones(shape,dtype=theano.config.floatX),\n",
    "                                          name=name,borrow=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let Ynp represent our matrix of partial, noisy observations\n",
    "#p = 0.03\n",
    "p      = 8*(maxit/(N*D))/(20)\n",
    "observe = np.random.permutation(np.arange(N*D))[1:int(np.floor(p*N*D))]\n",
    "Masknp = np.zeros(N*D)\n",
    "Masknp[observe] = 1\n",
    "observerow = np.random.randint(0, high=D, size=np.maximum(N,D))\n",
    "observecol = np.arange(N)\n",
    "Masknp = Masknp.reshape((D,N))\n",
    "Masknp[observerow, observecol] = 1\n",
    "#p = 0.08\n",
    "#Masknp = np.random.binomial(N, p, size=(N,D)).T\n",
    "#Masknp = stats.bernoulli.rvs(p, size=(D,N))\n",
    "#Masknp[1,:] = 1\n",
    "Mask   = T.as_tensor_variable(Masknp)\n",
    "M      = T.as_tensor_variable(Mnp)\n",
    "Y      = Mask*M\n",
    "zeroY  = T.as_tensor_variable(np.zeros((D,N)))\n",
    "zero2  = T.as_tensor_variable(np.zeros((D,D)))\n",
    "zero   = T.as_tensor_variable(np.zeros(D))\n",
    "st     = T.sum(T.neq(Y, zeroY), axis = 0)\n",
    "s      = st.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.057999999999999996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define variables \n",
    "W      = Shared((D,K), 'W')\n",
    "W.set_value(random_w_init)\n",
    "r      = Shared((K), 'r')\n",
    "r.set_value(random_r_init)\n",
    "Gamma  = Shared((1), 'Gamma')\n",
    "Gamma0 = Shared((1), 'Gamma0')\n",
    "c0     = Shared((1), 'c0')\n",
    "sigma  = Shared((1), 'sigma')\n",
    "\n",
    "t      = T.dscalar('t')\n",
    "\n",
    "#Define random variables for MVNscan component\n",
    "zY = srng.normal([D])\n",
    "zK = srng.normal([K])\n",
    "\n",
    "#For data given seqentially we need a different covariance matrix for each yn\n",
    "WWT=T.dot(W, W.T)\n",
    "Cov=Shared((D,D), 'Cov')\n",
    "Cov=WWT+sigma[0]*T.identity_like(WWT)\n",
    "#Cov = T.as_tensor_variable(covnp, name= 'Cov')\n",
    "\n",
    "#Define lists\n",
    "mParams = [W, r, Gamma, Gamma0, c0, sigma]\n",
    "\n",
    "#indexlist = Shared([maxit], 'indexlist')\n",
    "indexlist = theano.shared(value = np.zeros([maxit],dtype=np.int64),\n",
    "                                          name='indexlist',borrow=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define Functions for Langevin Step\n",
    "\n",
    "def logJointScanFn(n, logLikelihood, Y, Cov, s):\n",
    "    \n",
    "    idxs          = T.neq(Y[:,n], zero).nonzero()\n",
    "    y             = Y[:,n][idxs]\n",
    "    idxs2         = T.neq(T.outer(Y[:,n], Y[:,n]), zero2).nonzero()\n",
    "    littlecov     = Cov[idxs2].reshape((s[n], s[n]))\n",
    "    logLikelihood +=(-1/2.0)*T.log(T.nlinalg.Det()(littlecov))-(1/2.0)*T.dot(y.T, T.dot(T.nlinalg.MatrixInverse()(littlecov), y))\n",
    "    \n",
    "    return logLikelihood\n",
    "\n",
    "def LogJ(mParams, Y, Cov, s):\n",
    "\n",
    "    W, r, Gamma, Gamma0, c0, sigma = mParams\n",
    "    #LogJt0=time.clock()\n",
    "    results, updates = theano.scan(fn=logJointScanFn,\n",
    "                                   sequences = np.arange(N),\n",
    "                                   outputs_info=[dict(initial= np.float64(0) ,taps=[-1])],\n",
    "                                   non_sequences=[Y, Cov, s])\n",
    "    logJoint  = results[-1]\n",
    "    logJoint2 = ((D*Gamma*T.log(Gamma))[0]*r).sum()-(D*T.gammaln(Gamma[0]*r)).sum()+((Gamma[0]*r-1)*T.log(W)).sum()-(Gamma[0]*W).sum() + (Gamma0*T.log(c0)-K*T.gammaln(Gamma0/K)+(Gamma0/K-1)[0]*(T.log(r)).sum()-(c0[0]*r).sum()-Gamma-Gamma0-c0)[0]\n",
    "    logJoint  += logJoint2\n",
    "\n",
    "    return(logJoint)\n",
    "\n",
    "def adadelta2(loss_or_grads, params, learning_rate=1.0, rho=0.95, epsilon=1e-6):\n",
    "    \"\"\" \n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Zeiler, M. D. (2012):\n",
    "           ADADELTA: An Adaptive Learning Rate Method.\n",
    "           arXiv Preprint arXiv:1212.5701.\n",
    "    \"\"\"\n",
    "    grads = get_or_compute_grads(loss_or_grads, params)\n",
    "    updates = OrderedDict()\n",
    "\n",
    "    # Using theano constant to prevent upcasting of float32\n",
    "    one = T.constant(1)\n",
    "\n",
    "    for param, grad in zip(params, grads):\n",
    "        value = param.get_value(borrow=True)\n",
    "        # accu: accumulate gradient magnitudes\n",
    "        accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                             broadcastable=param.broadcastable)\n",
    "        # delta_accu: accumulate update magnitudes (recursively!)\n",
    "        delta_accu = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
    "                                   broadcastable=param.broadcastable)\n",
    "\n",
    "        # update accu (as in rmsprop)\n",
    "        accu_new = rho * accu + (one - rho) * grad ** 2\n",
    "        updates[accu] = accu_new\n",
    "\n",
    "        # compute parameter update, using the 'old' delta_accu\n",
    "        update = (grad * T.sqrt(delta_accu + epsilon) /\n",
    "                  T.sqrt(accu_new + epsilon))\n",
    "        #updates[param] = param - learning_rate * update\n",
    "        updates[param] = T.minimum(T.maximum((param - learning_rate * update).astype(theano.config.floatX), (1e-10)*T.ones_like(param)), 10*T.ones_like(param))\n",
    "\n",
    "        # update delta_accu (as accu, but accumulating updates)\n",
    "        delta_accu_new = rho * delta_accu + (one - rho) * update ** 2\n",
    "        updates[delta_accu] = delta_accu_new\n",
    "\n",
    "    return updates\n",
    "\n",
    "def get_or_compute_grads(loss_or_grads, params):\n",
    "    \"\"\"Helper function returning a list of gradients\n",
    "    \"\"\"\n",
    "    if any(not isinstance(p, theano.compile.SharedVariable) for p in params):\n",
    "        raise ValueError(\"params must contain shared variables only. If it \"\n",
    "                         \"contains arbitrary parameter expressions, then \"\n",
    "                         \"lasagne.utils.collect_shared_vars() may help you.\")\n",
    "    if isinstance(loss_or_grads, list):\n",
    "        if not len(loss_or_grads) == len(params):\n",
    "            raise ValueError(\"Got %d gradient expressions for %d parameters\" %\n",
    "                             (len(loss_or_grads), len(params)))\n",
    "        return loss_or_grads\n",
    "    else:\n",
    "        return theano.grad(loss_or_grads, params)\n",
    "    \n",
    "    \n",
    "#MVNormalScan constructs our estimate of the entire matrix using conditional multivariate normal\n",
    "\n",
    "def MVNormalScan_beta02(n, Y, Mask, Cov, W, zY, zK, s):\n",
    "    \n",
    "    #construct binaryY_unobs a vector of 1s and 0s where the ith coord is a 1 if we haven't seen the ith coord of y_n    \n",
    "    binaryY_unobs = T.eq(Y[:,n], zero)\n",
    "    #construct covariance of the observed entries where the rows/columns with nothing have a 1 on diag (so invertible)\n",
    "    idxs          = T.neq(Y[:,n], zero).nonzero()\n",
    "    y             = Y[:,n][idxs]\n",
    "    idxs2         = T.neq(T.outer(Y[:,n], Y[:,n]), zero2).nonzero()\n",
    "    littlecov     = Cov[idxs2].reshape((s[n], s[n]))\n",
    "    littlecov_inv = T.nlinalg.MatrixInverse()(littlecov)\n",
    "    \n",
    "    #sigma_observed     = T.outer(binaryY[:,n], binaryY[:,n])*Cov+(binaryY_unobs*T.identity_like(Cov))\n",
    "    sigma_unobs_obs         = (T.outer(binaryY_unobs, T.neq(Y[:,n], zero)))*Cov\n",
    "    idxs3                   = T.neq(sigma_unobs_obs, zero2).nonzero()\n",
    "    little_sigma_unobs_obs = sigma_unobs_obs[:,idxs].reshape((D, s[n])) \n",
    "    #sigma_observed_inv = T.nlinalg.MatrixInverse()(sigma_observed)\n",
    "    dummyY             = T.zeros(D)\n",
    "    \n",
    "    #draw the mean vector dummyY from N(0, WWT+sigma^2I) using computationally fast trick\n",
    "    dummy_results, dummy_updates= theano.scan(lambda prior_result, sigma, zY, W, zK: \n",
    "                                              T.sqrt(sigma)[0]*zY+T.dot(W,zK) + prior_result,\n",
    "                                              sequences=None,\n",
    "                                              outputs_info= T.zeros(D),\n",
    "                                              non_sequences=[sigma, zY, W, zK],\n",
    "                                              n_steps=R)\n",
    "    \n",
    "    dummyY       = dummy_results[-1]\n",
    "    dummyY       /= R\n",
    "    dummyY_obs   = dummyY[idxs]\n",
    "    dummyY_unobs = binaryY_unobs*dummyY\n",
    "    y_est        = dummyY_unobs + T.dot(T.dot(little_sigma_unobs_obs, littlecov_inv), (y-dummyY_obs))\n",
    "    y_est        = (y_est*binaryY_unobs)-(1e6)*Mask[:,n]\n",
    "    #y_est        = y_est*binaryY_unobs + Y[:,n]*Mask[:,n]\n",
    "    \n",
    "    return [y_est, sigma_unobs_obs, littlecov_inv]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrue = Y.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent\n",
    "\n",
    "counter = 1\n",
    "\n",
    "logJ = LogJ(mParams,Y,Cov, s)\n",
    "ParamUpdates2=adadelta2(-logJ,mParams)\n",
    "AdaDeltaStep2=theano.function(inputs=[], updates=ParamUpdates2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[ 0.97390015  0.52469324  0.49581771  1.4033288   0.85894179  0.74639297\n",
      "  1.69236316  0.65655201  1.68411802  0.91000296  0.90574836  1.67776819\n",
      "  1.16857063  1.21502785  0.42786266  1.83679483  1.03846537  0.27255364\n",
      "  0.67783298  0.04778285]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pylab import has clobbered these variables: ['var']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nnameW = 'WK' + str(KTRUE) +'.csv'\\nnamer = 'rK' + str(KTRUE) + '.csv'\\nnamecov = 'truecovsdK' + str(KTRUE) + '.csv'\\nnp.savetxt(nameW,  W.eval(), delimiter = ',')\\nnp.savetxt(namer,  r.eval(), delimiter = ',')\\nnp.savetxt(namecov,  w.dot(w.T), delimiter = ',')\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This serves to test how well, given whatever percent of data observed you set through variable \"p\", \n",
    "#we can estimate the covariance matrix\n",
    "% pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "limit = 1000\n",
    "\n",
    "outs, updts = theano.scan(AdaDeltaStep2,\n",
    "                              n_steps = limit)\n",
    "\n",
    "fn = theano.function(\n",
    "[],\n",
    "outs,updates = updts\n",
    ")\n",
    "fn()\n",
    "\n",
    "#plt.plot(ljlist)\n",
    "\n",
    "print(r.eval())\n",
    "\n",
    "\"\"\"\n",
    "nameW = 'WK' + str(KTRUE) +'.csv'\n",
    "namer = 'rK' + str(KTRUE) + '.csv'\n",
    "namecov = 'truecovsdK' + str(KTRUE) + '.csv'\n",
    "np.savetxt(nameW,  W.eval(), delimiter = ',')\n",
    "np.savetxt(namer,  r.eval(), delimiter = ',')\n",
    "np.savetxt(namecov,  w.dot(w.T), delimiter = ',')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.97390015  0.52469324  0.49581771  1.4033288   0.85894179  0.74639297\n",
      "  1.69236316  0.65655201  1.68411802  0.91000296  0.90574836  1.67776819\n",
      "  1.16857063  1.21502785  0.42786266  1.83679483  1.03846537  0.27255364\n",
      "  0.67783298  0.04778285]\n"
     ]
    }
   ],
   "source": [
    "print(r.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MainBandit2(count, indexlistnp, Mask, W, r, sigma, Gamma, Gamma0, c0, Y):\n",
    "    \n",
    "    limit = 1000\n",
    "    R     = 10\n",
    "    construct_lj_list = 0\n",
    "    construct_error_list = 0\n",
    "    tt = time.clock()\n",
    "    outs, updts = theano.scan(AdaDeltaStep2,\n",
    "                              n_steps = limit)\n",
    "\n",
    "    fn = theano.function(\n",
    "    [],\n",
    "    outs,updates = updts\n",
    "    )\n",
    "    fn()\n",
    "    \n",
    "    #[ljlist, errlist, W, r, sigma, Gamma, Gamma0, c0] = SGD(Y, limit,construct_lj_list, construct_error_list, W, r, sigma, Gamma, Gamma0, c0)\n",
    "    \n",
    "    tt = time.clock()\n",
    "    [y_estimate, sigma_u_o_scan, sigma_ob_inv_scan], updates=theano.scan(fn=MVNormalScan_beta02,\n",
    "                                              sequences=T.arange(N),\n",
    "                                              outputs_info=None,\n",
    "                                              non_sequences=[Y, Mask, Cov, W, zY, zK, st])\n",
    "    \n",
    "    \n",
    "    tt=time.clock()\n",
    "    y_estimate     = y_estimate.T\n",
    "    #print('y_estimate')\n",
    "    #print(y_estimate[1,:].eval())\n",
    "    [value, index] = T.max_and_argmax(y_estimate, axis=None, keepdims=False)   \n",
    "    mf             = T.flatten(Mask)\n",
    "    mf             = T.inc_subtensor(mf[index],1)\n",
    "    Mask           = mf.reshape((D,N))\n",
    "    #print('index')\n",
    "    #print(index.eval())\n",
    "    \n",
    "    #ON CPU IT MIGHT BE FASTER TO EVAL INDEX EACH ITERATION, BUT PERHAPS USE THEANO INDEXLIST FOR GPU \n",
    "    \n",
    "    #indexlist      = T.set_subtensor(indexlist[count], index)\n",
    "    indexlistnp.append(index.eval())\n",
    "    #print(r.eval())\n",
    "    return indexlistnp,Mask, W, r, sigma, Gamma, Gamma0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ind2sub(array_shape, ind):\n",
    "    #ind[ind < 0] = -1\n",
    "    #ind[ind >= array_shape[0]*array_shape[1]] = -1\n",
    "    rows = np.floor(ind / array_shape[1])\n",
    "    cols = ind % array_shape[1]\n",
    "    return (int(rows), int(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "count     = 0\n",
    "ratings   = [] \n",
    "R         = 5\n",
    "\n",
    "\"\"\"\n",
    "[indices, Mask_evolve, W_evolve, r_evolve, sigma_evolve, Gamma_evolve, Gamma0_evolve, c0_evolve], updates = theano.scan(fn=MainBandit2,\n",
    "                                               sequences = T.arange(5),\n",
    "                                               outputs_info = [indexlist,Mask, W, r, sigma, Gamma, Gamma0, c0],\n",
    "                                               non_sequences = Y)\n",
    "                                               \n",
    "indexlistnp = indices[-1].eval()\n",
    "\"\"\"\n",
    "indexlistnp = []\n",
    "for i in range(maxit):\n",
    "    [indexlistnp, Mask, W, r, sigma, Gamma, Gamma0, c0]  = MainBandit2(count, indexlistnp, Mask, W, r, sigma, Gamma, Gamma0, c0, Y)\n",
    "    count = count + 1\n",
    "    if i%10 == 0:\n",
    "        print(i)\n",
    "    #print(r.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#indexlist[-1].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#indexlistnp = indexlist[1:1000].eval()\n",
    "#indexlist[-1].eval()\n",
    "ratings = []\n",
    "initial_obs = np.multiply([Masknp!=0], Mnp).flatten()\n",
    "ratings = initial_obs[initial_obs!=0].tolist()\n",
    "for i in range(np.size(indexlistnp)):\n",
    "    [r,c] = ind2sub(np.shape(Mnp), int(indexlistnp[i]))\n",
    "    ratings.append(Mnp[r,c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexlistnp[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reward = np.cumsum(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#best = Mnp + 0.00001\n",
    "#best[Masknp] = 1e-6\n",
    "#best = best.flatten()\n",
    "#(np.multiply(Mnp, (1e-6)*Masknp)).flatten()\n",
    "best = Mnp.flatten()\n",
    "best.sort()\n",
    "best[:] =  best[::-1]\n",
    "best = np.cumsum(best)\n",
    "\n",
    "random_reward = np.zeros(np.size(Mnp.flatten()))\n",
    "for i in range(10):\n",
    "    #random = Mnp + 0.00001\n",
    "    #random[Masknp] = 1e-6\n",
    "    #random =  random.flatten()\n",
    "    #random = (np.multiply(Mnp, (1e-6)*Masknp)).flatten()\n",
    "    random = Mnp.flatten()\n",
    "    random = np.random.permutation(random)\n",
    "    random_reward += np.cumsum(random)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PLOT ACCUMULATED REWARDS\n",
    "\n",
    "% pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "length        = np.minimum(np.size(reward), np.size(best))\n",
    "reward        = reward[0:length]\n",
    "best          = best[0:length]\n",
    "random_reward = random_reward[0:length]\n",
    "\n",
    "plt.plot(best)\n",
    "plt.plot(reward)\n",
    "plt.plot(random_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indexlistnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#PLOT REGRET\n",
    "\n",
    "plt.plot(best-reward)\n",
    "plt.plot((best-random_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N*D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
